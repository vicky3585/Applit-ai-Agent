#!/bin/bash

# DeepSeek-Coder Installation Script for Applit
# Optimized for NVIDIA RTX 3060 (12GB VRAM)

set -e

echo "ðŸš€ Applit - DeepSeek-Coder Installation"
echo "========================================"
echo ""
echo "This will install DeepSeek-Coder-6.7B-Instruct"
echo "Best model for code generation on RTX 3060!"
echo ""

# Colors
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
BLUE='\033[0;34m'
NC='\033[0m'

# Check Ubuntu
if [[ ! -f /etc/lsb-release ]]; then
    echo -e "${RED}âŒ This script requires Ubuntu${NC}"
    exit 1
fi

echo -e "${GREEN}âœ“${NC} Ubuntu detected"

# Check NVIDIA GPU
if ! command -v nvidia-smi &> /dev/null; then
    echo -e "${RED}âŒ NVIDIA drivers not found${NC}"
    echo "Install drivers first: sudo ubuntu-drivers autoinstall"
    exit 1
fi

echo -e "${GREEN}âœ“${NC} NVIDIA GPU detected:"
GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader)
GPU_MEMORY=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader)
echo "   $GPU_NAME ($GPU_MEMORY)"

# Check VRAM (need at least 8GB)
VRAM_GB=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader | awk '{print int($1/1024)}')
if [ "$VRAM_GB" -lt 8 ]; then
    echo -e "${RED}âŒ Insufficient VRAM: ${VRAM_GB}GB (need 8GB+)${NC}"
    exit 1
fi

echo -e "${GREEN}âœ“${NC} VRAM: ${VRAM_GB}GB (sufficient)"

# Get project directory
PROJECT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
echo -e "${GREEN}âœ“${NC} Project: $PROJECT_DIR"

# Create/verify vLLM virtual environment
VLLM_VENV="$PROJECT_DIR/vllm-venv"

if [ -d "$VLLM_VENV" ]; then
    echo -e "${YELLOW}âš ${NC}  vLLM venv exists, using it"
else
    echo "Creating Python virtual environment..."
    python3 -m venv "$VLLM_VENV"
    echo -e "${GREEN}âœ“${NC} Virtual environment created"
fi

# Activate venv
source "$VLLM_VENV/bin/activate"

# Upgrade pip
echo "Upgrading pip..."
pip install --upgrade pip --quiet

# Install vLLM
echo "Installing vLLM (this may take 3-5 minutes)..."
pip install vllm --quiet
echo -e "${GREEN}âœ“${NC} vLLM installed: $(python -c 'import vllm; print(vllm.__version__)')"

# Install HuggingFace CLI
echo "Installing HuggingFace CLI..."
pip install huggingface-hub --quiet
echo -e "${GREEN}âœ“${NC} HuggingFace CLI ready"

# Login to HuggingFace
echo ""
echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo -e "${BLUE} HuggingFace Authentication Required ${NC}"
echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo ""

if ! huggingface-cli whoami &> /dev/null; then
    echo "You need a HuggingFace account to download models"
    echo ""
    echo "Steps:"
    echo "1. Create account: https://huggingface.co/join"
    echo "2. Get token: https://huggingface.co/settings/tokens"
    echo "   (Click 'New token' â†’ Select 'Read' access)"
    echo ""
    read -p "Press Enter when you have your token ready..."
    echo ""
    huggingface-cli login
else
    echo -e "${GREEN}âœ“${NC} Already logged in as: $(huggingface-cli whoami)"
fi

# Download DeepSeek-Coder
MODEL_NAME="deepseek-ai/deepseek-coder-6.7b-instruct"
echo ""
echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo -e "${BLUE} Downloading DeepSeek-Coder-6.7B      ${NC}"
echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo ""
echo "Model: $MODEL_NAME"
echo "Size: ~13GB (may take 5-20 minutes depending on internet)"
echo ""

# Check if already downloaded
CACHE_DIR="$HOME/.cache/huggingface/hub"
MODEL_CACHE=$(echo "$MODEL_NAME" | sed 's/\//__/g')

if [ -d "$CACHE_DIR/models--$MODEL_CACHE" ]; then
    echo -e "${YELLOW}âš ${NC}  Model already cached, skipping download"
    echo "   Location: $CACHE_DIR/models--$MODEL_CACHE"
else
    echo "Downloading... (progress shown below)"
    huggingface-cli download "$MODEL_NAME" --local-dir-use-symlinks False
    echo -e "${GREEN}âœ“${NC} Model downloaded successfully"
fi

# Optimize GPU settings based on VRAM
if [ "$VRAM_GB" -ge 12 ]; then
    GPU_MEM_UTIL="0.90"
    MAX_MODEL_LEN="8192"
    echo -e "${GREEN}âœ“${NC} Using 12GB+ profile (90% VRAM, 8K context)"
elif [ "$VRAM_GB" -ge 10 ]; then
    GPU_MEM_UTIL="0.85"
    MAX_MODEL_LEN="6144"
    echo -e "${YELLOW}âš ${NC}  Using 10-12GB profile (85% VRAM, 6K context)"
else
    GPU_MEM_UTIL="0.80"
    MAX_MODEL_LEN="4096"
    echo -e "${YELLOW}âš ${NC}  Using 8-10GB profile (80% VRAM, 4K context)"
fi

# Create startup script
STARTUP_SCRIPT="$PROJECT_DIR/start-deepseek.sh"
cat > "$STARTUP_SCRIPT" << EOF
#!/bin/bash
# DeepSeek-Coder vLLM Server Startup Script
# Auto-generated by install-deepseek.sh

echo "ðŸš€ Starting DeepSeek-Coder vLLM Server..."
echo ""

# Activate vLLM environment
source "$VLLM_VENV/bin/activate"

# Set GPU device (RTX 3060)
export CUDA_VISIBLE_DEVICES=0

# Start vLLM server
echo "Model: $MODEL_NAME"
echo "GPU: \$(nvidia-smi --query-gpu=name --format=csv,noheader)"
echo "Memory Utilization: ${GPU_MEM_UTIL} (${VRAM_GB}GB VRAM)"
echo "Max Context: ${MAX_MODEL_LEN} tokens"
echo "API Endpoint: http://0.0.0.0:8000/v1"
echo ""
echo "Press Ctrl+C to stop the server"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

python -m vllm.entrypoints.openai.api_server \\
  --model $MODEL_NAME \\
  --host 0.0.0.0 \\
  --port 8000 \\
  --tensor-parallel-size 1 \\
  --gpu-memory-utilization $GPU_MEM_UTIL \\
  --max-model-len $MAX_MODEL_LEN \\
  --dtype float16 \\
  --max-num-seqs 4 \\
  --trust-remote-code
EOF

chmod +x "$STARTUP_SCRIPT"
echo -e "${GREEN}âœ“${NC} Created: $STARTUP_SCRIPT"

# Update .env configuration
ENV_FILE="$PROJECT_DIR/.env"

if [ ! -f "$ENV_FILE" ]; then
    cp "$PROJECT_DIR/.env.example" "$ENV_FILE"
    echo -e "${GREEN}âœ“${NC} Created .env from template"
fi

# Update or add vLLM config
if grep -q "VLLM_MODEL_NAME" "$ENV_FILE"; then
    # Update existing
    sed -i "s|VLLM_MODEL_NAME=.*|VLLM_MODEL_NAME=$MODEL_NAME|" "$ENV_FILE"
    sed -i "s|AI_PROVIDER=.*|AI_PROVIDER=hybrid|" "$ENV_FILE"
    echo -e "${GREEN}âœ“${NC} Updated .env configuration"
else
    # Add new
    cat >> "$ENV_FILE" << EOF

# DeepSeek-Coder vLLM Configuration (auto-added)
AI_PROVIDER=hybrid
VLLM_API_BASE=http://localhost:8000/v1
VLLM_MODEL_NAME=$MODEL_NAME
CUDA_VISIBLE_DEVICES=0
EOF
    echo -e "${GREEN}âœ“${NC} Added vLLM config to .env"
fi

# Success summary
echo ""
echo -e "${GREEN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
echo -e "${GREEN}â•‘   ðŸŽ‰ Installation Complete!               â•‘${NC}"
echo -e "${GREEN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo ""
echo -e "${BLUE}Next Steps:${NC}"
echo ""
echo "1ï¸âƒ£  Start DeepSeek-Coder server (Terminal 1):"
echo "   ${STARTUP_SCRIPT}"
echo ""
echo "2ï¸âƒ£  Start Applit (Terminal 2):"
echo "   cd $PROJECT_DIR"
echo "   npm run dev"
echo ""
echo "3ï¸âƒ£  Monitor GPU usage (Terminal 3):"
echo "   watch -n 1 nvidia-smi"
echo ""
echo -e "${BLUE}Expected Performance:${NC}"
echo "   â€¢ Inference Speed: 8-12 tokens/sec"
echo "   â€¢ Code Quality: ~90% of GPT-4"
echo "   â€¢ Cost Savings: ~70% (hybrid) or 99% (vllm-only)"
echo ""
echo -e "${BLUE}Configuration:${NC}"
echo "   â€¢ Model: DeepSeek-Coder-6.7B-Instruct"
echo "   â€¢ Mode: Hybrid (vLLM + OpenAI GPT-4)"
echo "   â€¢ GPU Memory: ${GPU_MEM_UTIL} of ${VRAM_GB}GB"
echo "   â€¢ Max Context: ${MAX_MODEL_LEN} tokens"
echo ""
echo "ðŸ“– Full guide: docs/MODEL_RECOMMENDATIONS.md"
echo ""
