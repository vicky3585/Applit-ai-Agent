# vLLM Model Recommendations for RTX 3060 (12GB VRAM)

## üèÜ Best Model for Applit: DeepSeek-Coder-V2-Lite-Instruct

**Why DeepSeek-Coder is #1 for Applit:**
- ‚úÖ **Trained specifically for code generation** (16B params compressed to 6.7B effective)
- ‚úÖ **Optimized for JavaScript, TypeScript, Python, React** (Applit's stack)
- ‚úÖ **Fits perfectly on RTX 3060** (~7GB VRAM usage)
- ‚úÖ **Performance rivals GPT-4** on coding benchmarks (HumanEval: 81.1%)
- ‚úÖ **Fast inference** (8-12 tokens/sec on RTX 3060)
- ‚úÖ **Apache 2.0 license** (fully open, commercial use allowed)

---

## Model Comparison Chart

| Model | VRAM | Speed | Code Quality | Best Use Case |
|-------|------|-------|--------------|---------------|
| **ü•á DeepSeek-Coder-6.7B** | ~7GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **Code generation (BEST)** |
| **ü•à Qwen2.5-Coder-7B** | ~7GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Multi-language coding |
| **ü•â Llama 3.1 8B Instruct** | ~8GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | General chat/planning |
| CodeLlama 7B | ~7GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Legacy code tasks |
| Mistral 7B Instruct | ~7GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | General purpose |

---

## üöÄ Recommended Configuration for Applit

### Option 1: DeepSeek-Coder Only (Best Bang for Buck)

```bash
# .env configuration
AI_PROVIDER=hybrid
VLLM_MODEL_NAME=deepseek-ai/deepseek-coder-6.7b-instruct
VLLM_API_BASE=http://localhost:8000/v1
CUDA_VISIBLE_DEVICES=0
```

**Use Case:**
- Planning: DeepSeek-Coder (local GPU)
- Testing: DeepSeek-Coder (local GPU)
- Coding: OpenAI GPT-4 (API, for complex logic)

**Cost Savings:** ~70% (most tasks use free local GPU)

---

### Option 2: DeepSeek-Coder for Everything (99% Free)

```bash
# .env configuration
AI_PROVIDER=vllm
VLLM_MODEL_NAME=deepseek-ai/deepseek-coder-6.7b-instruct
VLLM_API_BASE=http://localhost:8000/v1
```

**Use Case:**
- All tasks use DeepSeek-Coder on local GPU
- No OpenAI API calls
- 99% cost savings

**Trade-off:** Slightly lower quality on very complex refactoring vs GPT-4

---

### Option 3: Dual Model Setup (Advanced)

Run **two vLLM servers** for specialized tasks:
- **Server 1 (Port 8000)**: DeepSeek-Coder for code generation
- **Server 2 (Port 8001)**: Llama 3.1 8B for planning/chat

**Use Case:** Maximum optimization, but requires 15GB+ VRAM (use --gpu-memory-utilization 0.5 on each)

---

## Performance Benchmarks

### DeepSeek-Coder-6.7B vs Competitors

| Benchmark | DeepSeek | Qwen2.5 | Llama 3.1 | GPT-4 |
|-----------|----------|---------|-----------|-------|
| HumanEval (Python) | 81.1% | 78.5% | 62.3% | 86.4% |
| MBPP (Python) | 70.2% | 68.9% | 55.1% | 75.8% |
| JS Code Gen | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| React Components | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Inference Speed | 8-12 tok/s | 9-13 tok/s | 7-10 tok/s | ~20 tok/s |

**Verdict:** DeepSeek-Coder delivers **90% of GPT-4 quality** at **0% cost** for coding.

---

## Installation Guide

### Quick Install (DeepSeek-Coder)

```bash
# Run automated setup
cd ~/Applit-ai-Agent
./scripts/install-deepseek.sh

# This will:
# 1. Create Python venv
# 2. Install vLLM
# 3. Download DeepSeek-Coder-6.7B (~13GB download)
# 4. Configure .env
# 5. Create startup script
```

### Manual Install

```bash
# 1. Activate vLLM environment
source ~/Applit-ai-Agent/vllm-venv/bin/activate

# 2. Login to HuggingFace (one-time)
huggingface-cli login
# Paste token from: https://huggingface.co/settings/tokens

# 3. Download DeepSeek-Coder
huggingface-cli download deepseek-ai/deepseek-coder-6.7b-instruct

# 4. Start vLLM server
python -m vllm.entrypoints.openai.api_server \
  --model deepseek-ai/deepseek-coder-6.7b-instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --tensor-parallel-size 1 \
  --gpu-memory-utilization 0.90 \
  --max-model-len 8192 \
  --trust-remote-code
```

---

## Optimization Tips for RTX 3060

### Memory Optimization

```bash
# For 12GB VRAM (RTX 3060):
--gpu-memory-utilization 0.90  # Use 90% of GPU memory
--max-model-len 8192            # Context length (balance speed/memory)

# If OOM (Out of Memory) errors:
--gpu-memory-utilization 0.80  # Reduce to 80%
--max-model-len 4096            # Shorter context
```

### Speed Optimization

```bash
# Enable tensor parallelism (RTX 3060 has 1 GPU)
--tensor-parallel-size 1

# Use FP16 precision (default, fastest)
--dtype float16

# Enable continuous batching
--max-num-seqs 8  # Process up to 8 requests concurrently
```

### Quality Optimization

```bash
# Higher precision (slower, better quality)
--dtype bfloat16  # Better numerical stability

# Temperature settings (in your API calls)
temperature: 0.2  # Lower = more deterministic (code)
temperature: 0.7  # Higher = more creative (planning)
```

---

## Troubleshooting

### Issue: Model download too slow

**Solution:** Use HF mirror or download manually:

```bash
# Set HuggingFace mirror (China users)
export HF_ENDPOINT=https://hf-mirror.com

# Or download via browser and place in cache
# Default cache: ~/.cache/huggingface/hub/
```

### Issue: Out of Memory during load

**Solution:** Reduce memory utilization:

```bash
--gpu-memory-utilization 0.75
--max-model-len 4096
```

### Issue: Slow inference (<5 tok/s)

**Possible causes:**
1. GPU throttling (check temperature: `nvidia-smi`)
2. CPU bottleneck (upgrade PyTorch: `pip install --upgrade torch`)
3. Other processes using GPU (close them)

**Solution:**

```bash
# Check GPU usage
nvidia-smi

# Kill competing processes
sudo fuser -v /dev/nvidia*
```

---

## Alternative Models (If DeepSeek doesn't work)

### Qwen2.5-Coder-7B (Second Best)

```bash
VLLM_MODEL_NAME=Qwen/Qwen2.5-Coder-7B-Instruct

# Pros: Excellent multi-language support
# Cons: Slightly slower than DeepSeek on JS/TS
```

### Llama 3.1 8B (General Purpose)

```bash
VLLM_MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct

# Pros: Great for chat/planning
# Cons: Not specialized for code
```

---

## Next Steps

1. ‚úÖ **Install DeepSeek-Coder** (recommended)
2. ‚úÖ **Start vLLM server**
3. ‚úÖ **Configure Applit for hybrid mode**
4. ‚úÖ **Test with a React app generation**
5. ‚úÖ **Monitor GPU usage and adjust settings**

**Enjoy local AI inference at 90% of GPT-4 quality with 0% cost!** üöÄ
