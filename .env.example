# Environment Configuration
# Copy this file to .env and update with your values

# Deployment environment: "replit" or "local"
DEPLOYMENT_ENV=local

# OpenAI API Key (required for AI chat)
OPENAI_API_KEY=your_openai_api_key_here

# Session secret for authentication (change in production!)
SESSION_SECRET=change-this-to-a-random-secret-in-production

# Database connection (PostgreSQL)
DATABASE_URL=postgresql://postgres:postgres@postgres:5432/webide

# Redis connection (for caching)
REDIS_URL=redis://redis:6379

# code-server (VS Code Web) configuration
CODE_SERVER_URL=http://code-server:8080
IDE_PASSWORD=admin

# AI Provider: "openai", "vllm", or "hybrid"
# IMPORTANT: vLLM is OPTIONAL - defaults to OpenAI if vLLM unavailable
# 
# - openai: Use only OpenAI API (RECOMMENDED - always works)
# - vllm: Try local vLLM → fallback to OpenAI if unavailable
# - hybrid: Try vLLM for planning/testing → fallback to OpenAI if unavailable
#
# For most users, "openai" provides the best experience.
# Local GPU inference is a cost-optimization feature for advanced users.
AI_PROVIDER=openai

# vLLM (Local GPU Inference) - OPTIONAL
# See docs/VLLM_OPTIONAL.md for setup instructions and troubleshooting
# 
# KNOWN ISSUES on Ubuntu 24.04:
# - vLLM 0.11.0: Memory errors with 7B models on 12GB GPUs
# - vLLM 0.6.3: pyairports dependency may fail to import
# - Recommendation: Use OpenAI only unless you have specific GPU requirements
#
# Uncomment and configure only if you have a working vLLM server:
# VLLM_API_BASE=http://localhost:8000/v1
# VLLM_MODEL_NAME=Qwen/Qwen2.5-Coder-7B-Instruct
# CUDA_VISIBLE_DEVICES=0

# Node environment
NODE_ENV=production
