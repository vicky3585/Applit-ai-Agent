# Environment Configuration
# Copy this file to .env and update with your values

# Deployment environment: "replit" or "local"
DEPLOYMENT_ENV=local

# OpenAI API Key (required for AI chat)
OPENAI_API_KEY=your_openai_api_key_here

# Session secret for authentication (change in production!)
SESSION_SECRET=change-this-to-a-random-secret-in-production

# Database connection (PostgreSQL)
DATABASE_URL=postgresql://postgres:postgres@postgres:5432/webide

# Redis connection (for caching)
REDIS_URL=redis://redis:6379

# code-server (VS Code Web) configuration
CODE_SERVER_URL=http://code-server:8080
IDE_PASSWORD=admin

# AI Provider: "openai", "vllm", or "hybrid"
# - openai: Use only OpenAI API (requires OPENAI_API_KEY)
# - vllm: Use only local vLLM server (requires vLLM running on GPU)
# - hybrid: Use vLLM for planning/testing, OpenAI for critical code generation (RECOMMENDED for cost + speed)
AI_PROVIDER=hybrid

# vLLM (Local GPU Inference) Configuration
# Start vLLM server: python -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-3.1-8B-Instruct
# Default port: 8000, API endpoint: /v1
VLLM_API_BASE=http://localhost:8000/v1
VLLM_MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct

# GPU Configuration (for NVIDIA GPUs)
# Set to your GPU device ID (0 for single GPU, "0,1" for multi-GPU)
CUDA_VISIBLE_DEVICES=0

# Node environment
NODE_ENV=production
